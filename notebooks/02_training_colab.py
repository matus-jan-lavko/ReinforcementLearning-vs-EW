# -*- coding: utf-8 -*-
"""RL_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qdNsQRk6j4hy6hj6mYYs6WCD_3sVL4NV

# RL Tests using FinRL
- This notebook is based on https://arxiv.org/abs/2011.09607 and edited to the needs of the research paper

Process:
  1. Loading up data
  2. Preprocessing data into a panel format and split into training a testing sample
  3. Engineering features
  4. Adding the covariance matrix states for each $t$ with a lookback (estimation period)
  5. For a set of hyperparameters $p_1, ... p_n$ of size $m \times 1$ there is a grid of feasible models $M$ which is of size $m^n$. We call this a soft search method. We deploy the soft search training a total number of $M$ models obtaining their training results. This is done for each of the model architectures to ensure that the resulting model is tuned optimally.
  6. We evaluate these models using a number of criteria selecting the best hyperparameter combination.
  7. We test this model on a holdout test set obtaining the actions and returns for the given model.
"""

from google.colab import drive
drive.mount('/content/gdrive')

!pip install finrl

import sys
sys.path.append("../FinRL-Library")
sys.path.append("/content/gdrive/My Drive")

import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
matplotlib.use('Agg')
import datetime
import time
import shutil
import itertools

from finrl.config import config
from finrl.preprocessing.preprocessors import FeatureEngineer
from finrl.preprocessing.data import data_split
from finrl.env.env_portfolio import StockPortfolioEnv

from finrl.model.models import DRLAgent
# from finrl.trade.backtest import BackTestStats, BaselineStats, BackTestPlot, backtest_strat, baseline_strat
# from finrl.trade.backtest import backtest_strat, baseline_strat

from utils import read_ohlcv, preprocess_custom_data, add_cov_matrix_states

config.TECHNICAL_INDICATORS_LIST

import os
if not os.path.exists("./" + config.DATA_SAVE_DIR):
    os.makedirs("./" + config.DATA_SAVE_DIR)
if not os.path.exists("./" + config.TRAINED_MODEL_DIR):
    os.makedirs("./" + config.TRAINED_MODEL_DIR)
if not os.path.exists("./" + config.TENSORBOARD_LOG_DIR):
    os.makedirs("./" + config.TENSORBOARD_LOG_DIR)
if not os.path.exists("./" + config.RESULTS_DIR):
    os.makedirs("./" + config.RESULTS_DIR)

!ls gdrive/MyDrive/datasets/Research

"""# Loading the dataset"""

DB_PATH = 'gdrive/MyDrive/datasets/Research/'
DB_PATH_RESULTS = 'gdrive/MyDrive/RL_Results/'
DATASET_NAME = '00_db_SPX_GROWTH50'

dc = read_ohlcv(DATASET_NAME,DB_PATH)

df = preprocess_custom_data(**dc,start_date = '10/15/2008')

fe = FeatureEngineer(
                    use_technical_indicator=True,
                    use_turbulence=True,
                    user_defined_feature = False)

df = fe.preprocess_data(df)

df = add_cov_matrix_states(df, lookback = 252)



train = data_split(df, '2008-01-01','2019-01-01')
trade = data_split(df, '2019-01-01', '2021-01-01')

stock_dimension = len(train.tic.unique())
state_space = stock_dimension
env_kwargs = {
    "hmax": 100, 
    "initial_amount": 1000000, 
    "transaction_cost_pct": 0.001, 
    "state_space": state_space, 
    "stock_dim": stock_dimension, 
    "tech_indicator_list": config.TECHNICAL_INDICATORS_LIST, 
    "action_space": stock_dimension, 
    "reward_scaling": 1e-4
}

e_train_gym = StockPortfolioEnv(df = train, **env_kwargs)
env_train, _ = e_train_gym.get_sb_env()

def soft_search(dict_params, model_name, agent,
                params_config, timesteps):
  
  tic = time.perf_counter()

  vals = list(dict_params.values())
  parameters = list(dict_params.keys())

  #multiple parameters
  if len(dict_params) > 1:
    param_permutation = list(itertools.product(*vals))
    #

  #single parameter case
  if len(dict_params) == 1:
    foo = itertools.chain(*test_params.values())
    param_permutation = [tuple((x, )) for x in foo]

  print(f'There are a total of {len(param_permutation)} of models to be trained.')
  for idx, run in enumerate(param_permutation):
    #train a model for each config

    for param_id in range(len(parameters)):
      #set each parameter
      print(f'Setting {parameters[param_id]} to {run[param_id]}.')
      params_config[parameters[param_id]] = run[param_id]
        
    #train model
    model = agent.get_model(model_name, model_kwargs = params_config)
    print(f'Training model number {idx + 1}.')

    trained_model = agent.train_model(model = model, tb_log_name = DATASET_NAME + '_' + model_name,
                                      total_timesteps = timesteps)
    del trained_model

  #save the tensorboard logs
  tb_dir = os.path.join('tensorboard_log', model_name)
  results_dir = os.path.join('gdrive', 'MyDrive', 'RL_Results', DATASET_NAME, model_name.upper() ,'search')

  if not os.path.exists(results_dir):
    os.makedirs(results_dir)

  assert os.path.exists(tb_dir)
  assert os.path.exists(results_dir)
  shutil.move(tb_dir, results_dir)
  #save models
  model_config = pd.DataFrame(param_permutation).to_csv(os.path.join(results_dir, 'models.csv'))


  print('-------------------------Finished!---------------------------')
  print('Models trained: ')
  print(param_permutation)
  toc = time.perf_counter()  
  print(f'Soft search done in {(toc - tic)/60 :.2f} minutes')

# model_name = 'a2c'
# tb_dir = os.path.join('tensorboard_log', model_name)
# results_dir = os.path.join('gdrive', 'MyDrive', 'RL_Results', DATASET_NAME, model_name.upper() ,'search')

# if not os.path.exists(results_dir):
#   os.makedirs(results_dir)

# assert os.path.exists(tb_dir)
# assert os.path.exists(results_dir)
# shutil.move(tb_dir, results_dir)

"""# A2C

## Parameter Search
"""

agent = DRLAgent(env = env_train)
test_params = {'learning_rate': [0.00001, 0.0001, 0.001, 0.01]}

soft_search(test_params, 'a2c', agent, config.A2C_PARAMS, timesteps = 60000)

"""## Tuned Model"""

config.A2C_PARAMS['learning_rate'] = 0.01

agent = DRLAgent(env = env_train)

model_a2c = agent.get_model(model_name= 'a2c', model_kwargs = config.A2C_PARAMS)

trained_a2c = agent.train_model(model=model_a2c, 
                                tb_log_name= DATASET_NAME + '_a2c',
                                total_timesteps=60000)

e_trade_gym = StockPortfolioEnv(df = trade, **env_kwargs)
df_return, df_actions = DRLAgent.DRL_prediction(model=trained_a2c, environment = e_trade_gym)

if not os.path.exists(DB_PATH_RESULTS + DATASET_NAME + '/'):
    os.makedirs(DB_PATH_RESULTS + DATASET_NAME + '/')
df_return.to_csv(DB_PATH_RESULTS + DATASET_NAME + '/A2C' + '/a2c_returns.csv')
df_actions.to_csv(DB_PATH_RESULTS + DATASET_NAME + '/A2C' + '/a2c_actions.csv')
del df_return
del df_actions

"""# PPO

## Parameter Search
"""

agent = DRLAgent(env = env_train)

test_params = {'learning_rate': [0.00001, 0.0001, 0.001, 0.01],
               'batch_size': [256, 512],
               'ent_coef': [0.001]}

soft_search(test_params, 'ppo', agent, config.PPO_PARAMS, timesteps = 80000)

"""## Tuned Model"""

config.PPO_PARAMS['learning_rate'] = 0.01
config.PPO_PARAMS['batch_size'] = 512
config.PPO_PARAMS['ent_coef'] = 0.001

agent = DRLAgent(env = env_train)
model_ppo = agent.get_model('ppo', model_kwargs = config.PPO_PARAMS)

trained_ppo = agent.train_model(model_ppo, tb_log_name = DATASET_NAME + '_ppo', total_timesteps=80000)

e_trade_gym = StockPortfolioEnv(df = trade, **env_kwargs)
env_trade, obs_trade = e_trade_gym.get_sb_env()

df_return, df_actions = DRLAgent.DRL_prediction(model=trained_ppo, environment = e_trade_gym)

if not os.path.exists(DB_PATH_RESULTS + DATASET_NAME + '/'):
    os.makedirs(DB_PATH_RESULTS + DATASET_NAME + '/')
df_return.to_csv(DB_PATH_RESULTS + DATASET_NAME + '/PPO' + '/ppo_returns.csv')
df_actions.to_csv(DB_PATH_RESULTS + DATASET_NAME + '/PPO' + '/ppo_actions.csv')
del df_return
del df_actions

"""# DDPG

## Parameter Search
"""

agent = DRLAgent(env = env_train)

test_params = {'learning_rate': [0.00005, 0.0001, 0.001],
               'batch_size': [128, 256, 512]}

soft_search(test_params, 'ddpg', agent, config.DDPG_PARAMS, timesteps = 60000)

"""## Tuned Model"""

config.DDPG_PARAMS['learning_rate'] = 0.001
config.DDPG_PARAMS['batch_size'] = 128

agent = DRLAgent(env = env_train)
model_ddpg = agent.get_model('ddpg', model_kwargs = config.DDPG_PARAMS)

e_trade_gym = StockPortfolioEnv(df = trade, **env_kwargs)
env_trade, obs_trade = e_trade_gym.get_sb_env()

trained_ddpg = agent.train_model(model = model_ddpg, tb_log_name= DATASET_NAME + '_ddpg', total_timesteps=60000)

e_trade_gym = StockPortfolioEnv(df = trade, **env_kwargs)
df_return, df_actions = DRLAgent.DRL_prediction(model=trained_ddpg, environment = e_trade_gym)

if not os.path.exists(DB_PATH_RESULTS + DATASET_NAME + '/'):
    os.makedirs(DB_PATH_RESULTS + DATASET_NAME + '/')
df_return.to_csv(DB_PATH_RESULTS + DATASET_NAME + '/DDPG' + '/ddpg_returns.csv')
df_actions.to_csv(DB_PATH_RESULTS + DATASET_NAME + '/DDPG' + '/ddpg_actions.csv')
del df_return
del df_actions

"""# SAC

## Parameter Search
"""

agent = DRLAgent(env = env_train)

test_params = {'learning_rate': [0.00001, 0.0001],
               'batch_size': [256, 512]}

soft_search(test_params, 'sac', agent, config.SAC_PARAMS, timesteps = 60000)

"""## Tuned Model"""

config.SAC_PARAMS['learning_rate'] = 0.0001
config.SAC_PARAMS['batch_size'] = 256

agent = DRLAgent(env = env_train)
model_sac = agent.get_model('sac', model_kwargs = config.SAC_PARAMS)

trained_sac = agent.train_model(model = model_sac,
                                tb_log_name = DATASET_NAME + '_sac',
                                total_timesteps = 60000)

e_trade_gym = StockPortfolioEnv(df = trade, **env_kwargs)
df_return, df_actions = DRLAgent.DRL_prediction(model=trained_sac, environment = e_trade_gym)

if not os.path.exists(DB_PATH_RESULTS + DATASET_NAME + '/'):
    os.makedirs(DB_PATH_RESULTS + DATASET_NAME + '/')

df_return.to_csv(DB_PATH_RESULTS + DATASET_NAME + '/SAC' + '/sac_returns.csv')
df_actions.to_csv(DB_PATH_RESULTS + DATASET_NAME + '/SAC' + '/sac_actions.csv')
del df_return
del df_actions

"""# TD3

## Parameter Search
"""

agent = DRLAgent(env = env_train)
config.TD3_PARAMS['buffer_size'] = 150_000

test_params = {'learning_rate': [0.005, 0.01],
               'batch_size': [256, 512]}

soft_search(test_params, 'td3', agent, config.TD3_PARAMS, timesteps = 60000)

"""## Tuned Model"""

config.TD3_PARAMS['buffer_size'] = 150_000
config.TD3_PARAMS['learning_rate'] = 0.005
config.TD3_PARAMS['batch_size'] = 512

model_td3 = agent.get_model('td3', model_kwargs = config.TD3_PARAMS)

trained_td3 = agent.train_model(model = model_td3,
                                tb_log_name = DATASET_NAME + '_td3',
                                total_timesteps = 60000)

e_trade_gym = StockPortfolioEnv(df = trade, **env_kwargs)
df_return, df_actions = DRLAgent.DRL_prediction(model=trained_td3, environment = e_trade_gym)

if not os.path.exists(DB_PATH_RESULTS + DATASET_NAME + '/'):
    os.makedirs(DB_PATH_RESULTS + DATASET_NAME + '/')
df_return.to_csv(DB_PATH_RESULTS + DATASET_NAME + '/TD3' + '/td3_returns.csv')
df_actions.to_csv(DB_PATH_RESULTS + DATASET_NAME + '/TD3' + '/td3_actions.csv')
del df_return
del df_actions

"""# Tensorboard"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
import os
# %load_ext tensorboard

DATASET_NAME

spath_a2c = os.path.join('gdrive/MyDrive/RL_Results/',DATASET_NAME, 'A2C', 'search', 'a2c')
spath_ddpg = os.path.join('gdrive/MyDrive/RL_Results/',DATASET_NAME, 'DDPG', 'search', 'ddpg')
spath_ppo = os.path.join('gdrive/MyDrive/RL_Results/',DATASET_NAME, 'PPO', 'search', 'ppo')
spath_sac = os.path.join('gdrive/MyDrive/RL_Results/',DATASET_NAME, 'SAC', 'search', 'sac')
spath_td3 = os.path.join('gdrive/MyDrive/RL_Results/',DATASET_NAME, 'TD3', 'search', 'td3')

path_a2c = os.path.join('gdrive/MyDrive/RL_Results/',DATASET_NAME,'A2C')
path_ddpg = os.path.join('gdrive/MyDrive/RL_Results/',DATASET_NAME,'DDPG')
path_ppo = os.path.join('gdrive/MyDrive/RL_Results/',DATASET_NAME,'PPO')
path_sac = os.path.join('gdrive/MyDrive/RL_Results/',DATASET_NAME,'SAC')
path_td3 = os.path.join('gdrive/MyDrive/RL_Results/',DATASET_NAME,'TD3')

"""## A2C"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir $spath_a2c

"""## PPO"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir $spath_ppo

"""## DDPG"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir $spath_ddpg

"""## SAC"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir $spath_sac

"""## TD3"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir $spath_td3

"""## Tuned Models"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir $path_a2c

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir $path_ppo

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir $path_ddpg

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir $path_sac

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir $path_td3

config.A2C_PARAMS

config.PPO_PARAMS

config.DDPG_PARAMS

config.SAC_PARAMS

config.TD3_PARAMS

